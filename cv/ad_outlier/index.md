# 4章 近傍法による異常検知
- ホテリングが有効なのは観測値が1点に集中してる時だけ
- その縛りもなく簡単なのが近傍法
- 距離の決め方が大事
- 「距離のリーマン計量」を最適化して近傍法をいい感じに
- 「マージン最大化近傍法」を紹介

## 4.1 k近傍法: 経験分布に基づく異常判定
- 「k-nearest neighbors algorithm」

## 4.1 要約
- 4.1.1
  - 「経験分布」を使って異常検知する
  - 近くにある点の数が規定数入る円の半径ε
  - デルタ関数なので経験した時だけ1になり他は0になる
  - 半径εが同じなら要素数kが少ないと異常度が高い
  - 要素数kが同じ数なら，半径εが大きい方が異常度が高い
  - 「k近傍法」は要素数kを固定して異常判定する方法
  - 「ε近傍法」は半径εを固定して異常判定する
  - 分布がいくつかの集団に分かれていてOKなのがホテリングよりもいい点
  - 塊具合の違いや次元数が増えて寄与度が減るといった弱点もある

- 4.1.2 「ラベルつきデータに対するk近傍法」
  - ラベルがある場合は，異常と正常で分けて分布を作る
  - 観測点x'について，正常である確率は近傍のkこのうち正常ラベルの点の割合になる
  - ベイズの定理から，異常度a(x')は正常と異常の対数比で表される
  - これを使うには，「近傍数k」と「異常判定の閾値a_th」を「1つ抜き交差確認法」を使って決める
  - 近傍法では「距離尺度」を決める必要がある
  - フツーはユークリッド距離を使う
  - 距離や異常度の計算方法を決めるには，「局所外れ値度」という手法が良い

## 4.2 要約「マージン最大化近傍法」
- 「計量学習」は分布の仕方によって距離の測り方を変える
- その一つに「マージン最大化近傍法」がある
- 4.2.1 「計量学習とは」
  - 「半正定値行列A」を決めることができれば分布にあった距離を決められる
  - リーマン幾何学では行列Aを「リーマン計量」という
  - この行列Aをデータから学習することを「計量学習」という
  - Aは回転や伸縮させることで最適化を狙っている
  - 同じラベルを密集させ，違うラベルをなるべく引き離すのが目標
- 4.2.2 「マージン最大化近傍法の目的関数」
  - ある一点に注目してk個の近傍「標的近傍N^(n)」を求めておく
  - 空間を歪め，異なるラベルの点をマージンの外に追い出す
  - マージンは幅を広げることができる
- 4.2.3 「勾配法による最適化」
  - 「リーマン計量A」を更新する際は普通の微分が定義できないので「劣勾配法」という
  - 「ブートストラップ法」で標本数の偏りを修正できる
  - 「勾配法で更新→分解→マイナスの固有値を0に置き換え」を繰り返す
- 4.2.4 「確率モデルとの関係」
  - 同じラベルの点を密集させながら，条件を破られている度合いをなるべく小さくしたい
  - 式を変換すると，第１項は同じものが出てきて，第２項は「ある種の正規化項」で，求めるのが難しいため代わりにマージン制約を使用したと解釈できる

- outlier か novelty
- ResNet, VGG
- 分類問題
- サンプルコード回す

# 5章 混合分布モデルによる逐次更新型異常検知
- 空調など複数の動作モデルからなる系の自然な定式化方法
- 「EM法」を使うことでパラメータ推定のための公式がわかる
- 時間変化にも対応できるような重みつき最尤推定も行う
- 5.1 「混合分布モデルとその逐次更新: 問題設定」
  - 動作モードとは，人間では走っている時や歩いている時，座っている時など異なる状態を指している
  - 「**混合モデル**」は違う動作モードは分布が変わると言うことを確率分布に示したもの
  - 第k番目の分布のパラメータを「θ_k」とかき，第k「**クラスター**」と呼ぶ。
  - π_kを第kクラスターの「**結合重み**」と呼ぶ。
  - Θについての対数尤度は，対数にΣがあり最適解が求められないので，「EM法」によりパラメータを推定する。
  - 「混合分布」の確率分布が正規分布の時，「**混合正規分布**」という。
  - 「逐次更新型」では時刻tの観測した標本のみを使い，それ以前のデータは参照せずパラメータΘを更新する。
  - 対数尤度の式は，$L(Θ|\mathcal{D})=\sum_{n-1}^{N}\ln\sum{k=1}^{K}π_k p_k(\vec{x}^{(n)}|\vec{\theta}_k)$ (5.2)
  - 手元の標本全部を使って学習する方法を「**一括型学習**」(**バッチ学習**)という
  - 標本を観測するたびにパラメータを更新する学習を「**逐次更新型学習**」(**オンライン学習**)と呼ぶ。

- 5.2 「イエンセンの不等式による和と対数関数の順序交換」
  - 対数の中に和がある場合の対処法
  - 和と対数の順番を入れ替えると，元の式以下の対数尤度になる
  - 入れ替え後が大きくなれば，全体も大きくなる

- 5.3 「EM法による重みつき対数尤度の最大化」
  - 「重みつき対数尤度の下限」を使いパラメータを推定する
    - **EM法**(expectation-maximization algorithm)
  - 「EM法」では元のパラメータ{π,µ,Σ}, イエンセンで導入した{q^(t)}の2種類のうち，片方を固定して片方の身を求める計算をする
  - 元のパラメータがわかっている仮定では，未知数は{q}のみになる
    - $q_k^{(n)}=\dfrac{π_k\mathcal{N}(\vec{x}^{(n)}|µ_k,Σ_k)}{\sum_{l=1}^{K}π_l\mathcal{N}(x^{(n)}|µ_l,Σ_l)}$ (5.7)
  - qは標本xが第kクラスターにいる確率を表し「帰属度」と呼ばれる
    - 「帰属度(membership weight)」,「負担率(responsibility)
- 5.4 「混合重みのスムージング」
  - 「帰属度」の情報が新しいため，これを前提に元のパラメータを求めることができる
  - 初期値の与え方が悪いと，少数派のクラスターが出てきて不安定になってしまう
  - 「多項分布のスムージング」と同様にπにゲタを履かせることが有効
  - 「ディリクレ事前分布」を事前に足しておく
- 5.5 「重みの選択と逐次更新型異常モデル」
  - 最終目標は前の時刻のデータは忘れて，現時刻のデータのみでモデルのパラメータを更新すること
  - **忘却率**と呼ばれる定数βを導入する
  - EM法を使った更新は実務的な異常検知に機械学習の理論が導入されるきっかけとなった歴史的な手法
  - ホテリングのT^2法の欠点を改良したものとして実応用上最も広く使われているものの1つ 


# 6章 サポートベクトルデータ記述法による異常検知
- 第2章：ホテリングのT^2法では全データが1つの正規分布に従うと仮定しモデルを作った
- 第4, 5章：単一の分布ではなく，着目点の周りの局所的なデータの散らばりに着目してモデルを作った
- 第6章：「カーネルトリック」で分布の濃淡を間接的に表現する手法
- 6.1節「データを囲む最小の球」
  - ラベルなしデータの集まりをひとかたまりとして表現したい
  - 標本全体を球で囲み，はみ出したものを異常とするのがそれっぽい
  - 「ホテリングT^2法」はこれを多変量正規分布＋マハラノビスで実現した
  - データを全て含むような球の半径を遊びをペナルティとしてできる限り小さくする
    - $\min_{R^2,b,u} \{R^2+C\sum_{n=1}^{N}u^{(n)}\}subject to ||c^{(n)}-b||^2<=R^2+u{(n)}$
    - 異常度は球からはみ出した距離と定義できる
    - この距離が異常か判定するためには問題を解くと，異常と正常を分ける球が数少ない標本に左右されることがわかる
    - 僅かな標本が球面を支えていることから「サポートベクトルデータ記述法」と呼ばれる
  - 6.2節「双対問題への変換とカーネルトリック」
    - 級を小さくする問題は複雑なので，「双対問題」という別の問題に変換して非線形な制約をなくす方法を使う
    - 変換すると，次の式になり既存のプログラムで解ける簡単な形になる
      - $\max_{α}\{\sum_{n=1}^{N}\alpha_n K_{n,n}-\sum_{n,n'=1}^{N}\alpha_n\alpha_{n'}K_{n,n'}\}subject to 0 <=\alpha_n <=C (n=1,...,N)$
    - 最適化の手法には，SMO法と双対座標降下法がある
    - 元の標本x^(n)に内積Kのみを通して依存しており，RBFカーネルを使って座標を置き換えると，この内積によって非線形変換されたことに対応する
    - 上のように，内積を与えることで非線形変換をしたことにする考え方を**カーネルトリック**という
    - 内積として与える関数K(x,x')をカーネル関数という
      - どういう非線形変換したことになるかは6.4
  - 6.3節「解の性質と分類」
    - 最適解α*が求まったら，KKT条件を逆に遡って遡って元の解を求める
    - αが0でもCでもない場合，u=0となり，R^2=||x-b||が成り立つ
    - xがピッタリ球面の上に乗っており，支えているように見えることから**サポートベクトル**と呼ばれる
    - α=0の時，u=0, R²<||x-b||なのでα=0となる標本は球の内部にある
    - C<1に対してα＝Cの時，u>0となり，球の外にある。uは遊びとして導入したものなので，この場合もサポートベクトルと呼ぶ
    - C>=1ならすべての標本は球の上か内部にあるので，Cを1未満の正値にすることで球の外に出る標本の数を制御できる
    - 双対問題を解いて得られたα_n*によっvて，球の大きさも異常度も表現できる(6.13,6.14)
  - 6.4節「データクレンジングへの適用例」
    - 「サポートベクターデータ記述法」にカーネルトリックを加えることでドーナツ型に変形している
    - データにおいてノイズとみなされる標本を抽出して除去する作業をデータクレンジングという
    - どのような非線形変形がされどんな球が出来上がるか想像しにくい


# 7章「方向データの異常検知」
- 観測値の規格化・標準化はよく行われる
- 1ページの文書と100ページの文書を比べるとき，単語の頻度ベクトルを規格して眺める
- この時，正規分布で扱うべきではない
- 方向データ＝長さが揃ったベクトルからの異常検知を考える
- フォンミーゼス・フィッシャー分布が中心的な役割
- 7.1節「長さが揃ったベクトルについての分布」
  - 長さが1だと，かけても1なことから方向だけをもつベクトルを集めた，**方向データ**と呼ばれる
  - ベクトルの数が大きい時，先端が球面に作る濃淡は正規分布ではなく，フォンミーゼス。フィッシャー分布が最も自然である
  - **平均方向**と**集中度**という二つのパラメータを持っている
    - $\mathcal{M}(\boldsymbol{x}|\boldsymbol{\mu,\kappa})=\dfrac{\kappa^{\frac{M}{2}-1}}{(2\pi)^\frac{M}{2}I_{\frac{M}{2}-1}(\kappa)}\exp(\kappa\boldsymbol{\mu}^\top\boldsymbol{x})$
  - 平均方向μも単位ベクトルである
  - 分母のIは第1種変形ベッセル関数
  - Mを固定しκを動かして，xとμ(観測標本と平均)の間の角度θを見ると，κが大きいほどθのばらつきが小さくなる
  - つまり，集中度κは文字通り平均方向への集中度合いを表す
- 7.2節「平均方向の最尤推定」
  - $\mathcal{M}$に含まれる2つのパラメータμ, κをデータDから決める
  - 計算により，μの最尤推定値が方向データxの標本平均の長さを1にしたもの$\dfrac{m}{\sqrt{m^\top m}}$となる
- 7.3節「方向データの異常度とその確率分布」
  - x'の異常度は，平均と標本x'が似ているほど異常度が小さくなるように，
    $a(x')=1-\hat{\mu}^\top x'$と表せる
  - これは，κが十分に大きい時，カイ二乗分布に近似できる
  - κは1-μxを求め，カイ二乗分布に当てはめることで求める
- 7.4節「積率法によるカイ2乗分布の当てはめ」
  - 異常度a(x)を計算すると，カイ二乗分布に従う独立した標本とみなせる
  - カイ二乗分布パラメータを推定する方法として**積率法，モーメント法**がある
  - 積率：確率変数のべき乗の期待値
  - 積率をカイ二乗分布のパラメータで表現し，データから求められた数値と等置することで求める
  
# 8章「ガウス過程回帰による異常検知」
- 入力xから出力yの関数fを推定するモデル
- 非線形に推定できることが特徴
- 入力と出力の対が観測できる時に有効
- 応答曲面，回帰曲線という形で入出力の関係をモデル化する
- 8.1節「入出力がある場合の異常検知の考え方」
  - 入力と出力が両方ある時，与えられた入力に対して出力が異常でないかを見るのが自然(**応答異常**)
  - **回帰**は，xとyの関係を表す関数が未知の状態で，関数とノイズの両方を学習し，出力の確率分布を求める問題。
  - **ガウス過程回帰**は汎用性がかなり高い非線形回帰手法
  - 回帰に加えて異常度や閾値の適切な設定が求められる
- 8.2節「ガウス過程の観測モデルと事前分布」
  - ガウス過程回帰では，特定の分布を過程せずに，確率モデルの形で構築するのが目標
  - モデルは，観測時のノイズを表す確率モデルと，応答曲面のなめらかさを表現する事前分布からなる
  - ノイズのモデルは，fを中心に分散σ^2でバラつくと考えられる
  - 予測分布は，データをもとにxに対するyを確率的に予測する式から分布を作る。ガウス過程回帰の目標
  - ガウス過程は本質的には関数の関数としてのばらつきをモデル化したもの
    - いろんな関数として見ることができる！っていう意味？
  - xが近いとf(x)も近いと判断されている
  - 入力ごとにパラメータを与えるが，xが近いとyも近いという事前分布があるので，実質のパラメータは増えすぎない
  - 異常度はマハラノビス距離＋予測分布が入力xに依存した状態で非線形に求められる
  - 分散σ^2を決める際は，「周辺尤度」を最大にするように選べば良い。
  - データが与えられた時の分散の期待値「エビデンス」を最大化することでパラメータを決定することを「第2種最尤推定」という。(経験ベイズ法)


# 9章「部分空間法による変化検知」
- 点データの異常ではなく，流れの中の変化を検知する部門
- 「累積和」「スライド窓」という考え方で複数の異常度やデータをまとめる
- 「特異スペクトル変換法」は変化検知の問題でノイズに強く重要な手法である。

- 9.1節「累積和法: 変化検知の古典技術」
  - 化学プラントで液体の濃度の異常を見る時，1回ごとの異常値ではなく，流れを見て異常かどうかを判断したい
  - 何かの量ζが正常時にμをとるとき，平均μ，分散がσ^2なζについての正規分布Nになるはず。
  - 異常な時はνだけ増えることを知っていれば，そのモデルは平均μ+νの正規分布Nになるはず。
  - 上二つの確率分布を比較することで，現在の状態がどちらに近いかを検知できる。
    - 時刻tでどれだけ変化しているかを表す「**変化度**」は単発の標本を束ねる方法がある。
  - 式(9.1)は変化度を0を基準(対数だから)として正常の正規分布に対して，異常の正規分布が何倍かを定義している。
  - この変化度がプラスの時のみ累積させて(「**(上側)累積和**」)，**シューハート管理図**で時系列的に監視する。
  - 変化度がマイナスの時に累積する場合は「**下側累積和**」という。
  - 上振れと下振れの目安を決め方が実用上の困難なポイントとなる。

- 9.2節「近傍法による異常部位検出」
  - 異常部位を見つけることを考える
  - 窓をつけてまとまりを1つのベクトルとして考え，外れ値検出として異常部位を検出する方法がある
  - 長さがMの「**スライド窓**」によって，時系列データがM次元のベクトル「**部分時系列**」に設定される
  - ｋ＝1の近傍法により，異常度を，球の半径εの対数の定数倍で表すことができる
  - ベクトルは独立ではなく，特に近い時間のベクトルはほとんど等しく「**自己一致**，**自明な一致**」と呼ばれる
  - 「部分時系列同士の類似度」を計算するために，時間のずれを許すような「**動的時間伸縮法**」を利用する。
  - データマイニングでは，時系列を文字列に変換する「**集約記号近似**(SAX)」も研究されている

- 9.3節「変化検知問題と密度比」
  - スライド窓でベクトルを作るだけでは，調整が手作業で必要なので自動化に相応しくない。
  - 変化検知は，前と今でどのくらい状況が違うかを検知することで，2つの確率分布を推定し，相違度を計算する問題と定義できる。
  - 式1.2の単一の標本に対する異常度を拡張して，pによる期待値を計算する

- 9.4節「特異スペクトル交換法」
  - 密度比をデータから推定する
  - pは過去の「特徴的な波形」を表す単位ベクトルu，密集度κの時のFF分布
  - p'は現在の「特徴的な波形」を表す単位ベクトルq,密集度κの時のFF分布
  - データの部分時系列で典型的な波形は，vの内積1の制限でXvのノルムが最大になれば良い。
  - 部分系列の行列Xを**左特異ベクトル**u,**右特異ベクトル**v，**特異値**√γに分解することをXの「**特異値分解**」という
  - 特異値分解で，特異値√γの数をXの「**階数**」よりも小さくすると，人気のないz軸は無視され，式9.15は近似式とみなされる
  - つまり，「**特異値分解によるXの低階数近似はノイズ除去をやっているのと同じ**」
  - 過去のデータ行列Xを「**履歴行列**」と呼び，現在のデータ行列Zを「**テスト行列**」と呼び，「**ラグ**L」は，その「相互位置」を決める。
  - XとZに特異値分解を行い，特異値の高い左特異ベクトルをまとめたものがそれぞれUとQで，それぞれの列ベクトルではられる空間を「主部分空間」という。

- 9.5節「ランちょす法による特異スペクトル変換の高速化」
  - 変化検知に特化した特異スペクトル変換は、計算が重たいのが難点なので、一部の特異値分解の計算を省略して計算を効率化する。
  - M次元対称行列であるCを3重対角行列に変換することで作られる漸化式を、「**ランチョス法**」という。



